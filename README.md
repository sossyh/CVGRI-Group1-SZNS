# CVGRI-Group1
# Application of Diffusion Models in Medical Image

## 1. Introduction 

Given the recent advancements in computer vision, diffusion models have garnered significant attention within the field of medicine. This survey aims to provide a comprehensive overview of diffusion models in this domain, serving as a valuable resource for researchers navigating this rapidly evolving area.

## 1.1. Background 

Diffusion models are generative models that learn to generate data(image) by simulating a process that gradually adds noise to data (forward diffusion process) and then reverses it to recover the original data (reverse diffusion process).
Forward Diffusion Process: Data is progressively corrupted by Gaussian noise over time, transforming it into pure noise.
Reverse Diffusion Process: A neural network is trained to denoise step by step, starting from noise, to recreate the data.

## 2. Literature Review (Applications) 

### 2.1. Image enhancement

* Low light image enhancement
* Imbalance and low diversity samples image augmentation
* MRI reconstruction
* Image restoration: super resolution, blurring and inpainting
* FAB Adaptive image enhancement

### 2.2. Image synthesis 

* Addressing Class Imbalance: Diffusion models can generate diverse and realistic synthetic images of rare pathologies, mitigating the impact of class imbalance in medical datasets.

* Enhanced Medical Education: Diffusion models enable the creation of a limitless supply of unique synthetic medical images for educational purposes, improving training and practice.

* Mitigating Data Privacy Concerns: Synthetic medical images generated by diffusion models provide a privacy-preserving alternative to real patient data for research and development.

### 2.3. Image Segmentation

Medical image segmentation plays a pivotal role in modern healthcare by enabling the accurate identification and delineation of anatomical structures, such as organs, tumors, and lesions. This process is fundamental in clinical settings for diagnosing diseases, planning surgeries, and monitoring treatment outcomes. Accurate segmentation allows clinicians to measure tumor size and growth, identify regions of necrosis, and understand the spatial relationship between abnormal and healthy tissues, ultimately improving clinical decision-making and patient outcomes. Techniques like MRI, CT, and X-ray imaging rely heavily on segmentation to provide detailed insights into a patient's condition, assisting in tasks such as detecting early-stage cancers, outlining surgical margins, and guiding radiation therapy planning.

Despite its importance, achieving accurate segmentation in medical imaging remains challenging due to several factors. Medical images often exhibit low contrast between different types of tissues, making it difficult to distinguish between normal and abnormal regions. Noise from imaging equipment, patient motion, and environmental factors can degrade image quality, further complicating the segmentation task. Additionally, irregular boundaries and overlapping structures—such as tumors blending into surrounding tissues—pose significant challenges for traditional segmentation algorithms. Artifacts introduced during image acquisition, such as metal implants causing distortions in MRI or CT scans, can lead to inaccurate segmentations that affect downstream clinical decisions.

Traditional segmentation methods, including thresholding, region-growing techniques, and machine learning models like U-Net and fully convolutional networks (FCNs), have made significant advancements but still face limitations when dealing with noisy, low-contrast, or ambiguous images. These methods often require extensive preprocessing, hand-crafted features, and large annotated datasets to achieve reliable performance. Furthermore, they can struggle to generalize to diverse imaging conditions and patient populations, leading to suboptimal outcomes in real-world clinical settings.

Diffusion models have emerged as a promising alternative due to their ability to handle noise and generate robust, high-quality segmentation maps even in complex scenarios. By employing a probabilistic denoising process, diffusion models iteratively refine noisy inputs to produce structured outputs, making them well-suited for medical applications where precision is critical. Unlike traditional methods, which rely on deterministic mappings between input and output, diffusion models learn to model the entire distribution of possible segmentations, allowing them to generate multiple plausible solutions and handle uncertainty effectively [MedSegDiff 2022; BerDiff 2023].

The core strength of diffusion models lies in their ability to progressively transform noisy or incomplete data into accurate segmentations through a series of iterative denoising steps. Starting with an initial noisy version of the input, these models apply learned denoising transformations that gradually remove noise and reconstruct the underlying anatomical structures. This iterative approach ensures that even in cases of severe noise or artifacts, diffusion models can recover critical details that are essential for accurate clinical assessment.

Moreover, the probabilistic nature of diffusion models allows them to account for variability in image appearances across different patients, imaging devices, and clinical settings. This adaptability makes them particularly valuable in applications where high inter-patient variability is observed, such as tumor segmentation, where tumor shapes, sizes, and appearances can vary significantly. By learning to generate segmentations that reflect the inherent variability in medical images, diffusion models provide clinicians with a more comprehensive understanding of patient-specific conditions and potential treatment options.

The integration of diffusion models into medical workflows is further supported by their ability to handle ambiguous segmentation tasks. In many cases, ground truth labels provided by experts may vary due to differences in interpretation or imaging conditions. Diffusion models address this challenge by generating multiple plausible segmentations, offering clinicians a range of options to consider when making diagnostic or therapeutic decisions. This feature is particularly important in complex cases, such as brain tumor segmentation, where precise delineation of tumor boundaries is critical but often subject to uncertainty.

Overall, diffusion models represent a significant advancement in medical image segmentation, offering improved robustness, adaptability, and accuracy compared to traditional methods. Their ability to overcome noise, handle ambiguity, and produce high-quality segmentation maps positions them as a powerful tool for enhancing diagnostic precision and optimizing patient care in modern healthcare settings.

#### 2.3.1. Key Applications of Diffusion Models in Medical Image Segmentation

##### 2.3.1.1. Noise-to-Structure Mapping

One of the key strengths of diffusion models is their remarkable ability to map noisy or degraded medical images to accurate and well-defined segmentations. Medical imaging systems are prone to various types of noise, which can result from imaging artifacts, low signal-to-noise ratios, motion during scans, and limitations in the imaging technology itself. This noise not only deteriorates image quality but also complicates the task of accurately identifying anatomical structures. Traditional segmentation methods often fail to deal with such challenges, leading to inaccurate or incomplete segmentation outputs. However, diffusion models stand out by effectively handling noisy inputs through their iterative denoising mechanism.

The noise-to-structure mapping capability of diffusion models lies in their ability to progressively refine noisy data by removing random variations and enhancing meaningful features. For example, in cases of MRI brain scans affected by low contrast or signal degradation, diffusion models can reconstruct precise and clinically relevant tumor boundaries. This reconstruction occurs as the model learns the underlying anatomical patterns and structures during training, allowing it to differentiate between noise and important features within the medical image. By leveraging probabilistic inference, diffusion models ensure that the reconstructed output is not only denoised but also anatomically consistent, making them highly reliable in clinical applications.

Furthermore, this capability is particularly important in scenarios involving low-resource settings or emergency situations, where obtaining high-quality scans may not always be feasible. The robustness of diffusion models in dealing with suboptimal inputs makes them valuable for practical use in diverse medical environments. They enable healthcare professionals to make informed decisions based on accurate segmentation results, even when imaging conditions are less than ideal. For example, in emergency trauma cases requiring immediate imaging and analysis, diffusion models can quickly denoise scans and provide reliable segmentations, facilitating faster diagnoses and treatment planning [Ambiguous Medical Image Segmentation 2023].

##### 2.3.1.2. Generative Enhancement

Diffusion models are not limited to producing a single deterministic segmentation output. Instead, they can generate multiple plausible segmentations by sampling from the learned distribution. This is crucial in cases where there is uncertainty in the ground truth, such as when multiple radiologists provide differing annotations for the same image [BerDiff 2023].

This ability to generate diverse outputs stems from the probabilistic nature of diffusion models, allowing them to model multiple possible representations of the same medical structure. For example, in tumor segmentation tasks, precise boundary delineation can vary significantly among experts due to the complexity of tumor shapes, adjacent tissue involvement, and varying imaging conditions. By sampling different points along the learned distribution, diffusion models produce alternative segmentation proposals, each of which may capture a valid interpretation of the region of interest.

This diversity of outputs is particularly valuable when dealing with ambiguous or uncertain cases, as it provides a spectrum of possible solutions instead of relying on a single, potentially biased result. The generated segmentations can then be used to develop consensus-based systems, which combine and refine multiple plausible outputs to produce a more robust final segmentation. For instance, tumor boundaries identified by different radiologists can be reconciled through an ensemble of diffusion model outputs, reducing inter-observer variability and ensuring greater diagnostic accuracy [BerDiff 2023].

Moreover, generative enhancement facilitates better decision-making by providing clinicians with a range of plausible outcomes to consider during treatment planning. In cases where one segmentation result may miss critical details or fail to capture subtle features, the availability of multiple proposals ensures that important diagnostic information is not overlooked. This approach has been successfully applied in complex scenarios like brain tumor segmentation, where overlapping regions and irregular shapes make precise delineation challenging. By integrating diverse outputs, healthcare providers can prioritize treatment strategies that are robust to uncertainty and variability in segmentation [BerDiff 2023].

##### 2.3.1.3. Multi-Modal Segmentation

Many medical imaging tasks involve integrating information from multiple imaging modalities to improve segmentation accuracy and provide a holistic view of the anatomical structures. For example, MRI scans are highly effective in visualizing soft tissues, making them ideal for tasks like brain and muscle segmentation, while CT scans excel in capturing dense structures like bones. Combining data from these modalities ensures that the strengths of each imaging type are leveraged to produce more comprehensive and accurate segmentation results [MedSegDiff 2022].

Diffusion models have proven highly effective in learning cross-modal features due to their capacity to encode complex relationships between different imaging sources. During the learning process, these models identify complementary patterns from each modality, integrating soft-tissue and hard-tissue information into a unified segmentation map. For example, in the segmentation of head and neck cancers, MRI may highlight soft-tissue abnormalities, while CT helps delineate bony structures, resulting in a precise definition of tumor boundaries and surrounding anatomy when combined through diffusion modeling.

Incorporating multi-modal information allows diffusion models to address several limitations inherent to individual imaging techniques. For instance, while MRI provides excellent contrast in soft tissues, it often struggles to capture fine bony details, a gap that CT imaging can fill. Diffusion models overcome these modality-specific weaknesses by blending information from both modalities, thereby improving segmentation outcomes and reducing errors [MedSegDiff 2022].

Moreover, multi-modal segmentation is particularly beneficial in complex clinical cases such as brain tumor resection planning, where precise delineation of the tumor, surrounding tissues, and adjacent bone structures is critical. By integrating data from multiple imaging modalities, diffusion models can assist surgeons by offering a more detailed and accurate anatomical map, enabling better surgical precision and minimizing the risk of damaging healthy tissues.

The multi-modal approach also facilitates effective decision-making in radiation therapy planning. Accurate segmentation of both target tumors and healthy organs is essential to maximize radiation dose delivery while sparing healthy tissues. Diffusion models that incorporate multi-modal data provide radiation oncologists with enhanced segmentation maps that highlight critical structures, allowing for optimized treatment planning and better patient outcomes. This approach has been demonstrated to be effective in cases such as liver and lung cancer, where precise targeting is required to avoid damage to adjacent organs [MedSegDiff 2022].

By integrating multi-modal imaging, diffusion models have significantly advanced segmentation accuracy and consistency, contributing to improved diagnostic workflows and treatment efficacy in complex medical cases.

## Challenges and Limitations 

* Scalability: Limited generalization across diverse datasets.
* Computational Overhead: High resource requirements.
* Interpretability: Outputs lack transparency and clinical trust.
* Validation: Minimal real-world testing on clinical datasets.
* Dimensionality Reduction Challenge: Diffusion models may not be as effective as other models (like VAEs and GANs) in learning meaningful representations in their latent space.
* Slower Generation: Diffusion models generally have a slower generation process compared to some other generative models.
* Sensitive to noise scheduling

## Conclusion 

Diffusion models are transforming medical image reconstruction, enhancement, synthesis, and segmentation
Challenges persist in scalability, computational efficiency, and clinical applicability.
### Future work
* Automated Noise Schedule Optimization - Develop algorithms to automatically optimize the noise schedule during training, reducing the need for manual tuning. Since diffusion models are sensitive to noise scheduling 
* Meta learning approaches
* Combine diffusion models with other generative models such as GANs to increase interpretability 
* Exploring Diverse Modalities: Diffusion models have the potential to be applied to a wider range of medical imaging modalities beyond commonly used CT and MRI.
* Representation Learning: Diffusion models may struggle to learn semantically meaningful representations in their latent space, hindering tasks like image reconstruction and semantic interpolation.
* Transformer Architectures: While promising, the use of transformer models in diffusion models is still in its early stages, requiring further research to fully understand their capabilities.
* Privacy Concerns: The use of AI image synthesis models, including diffusion models, in medical imaging raises concerns regarding patient privacy and potential copyright violations.
* Reverse Process with Reinforcement Learning: Reinforcement learning can be employed to optimize the inverse process of diffusion models, potentially improving their performance and efficiency.

## References 

[Wu, J., Fu, R., Fang, H., Zhang, Y., Yang, Y., Xiong, H., ... & Xu, Y. (2024, January). Medsegdiff: Medical image segmentation with diffusion probabilistic model. In Medical Imaging with Deep Learning (pp. 1623-1639). PMLR.](https://proceedings.mlr.press/v227/wu24a.html)

[Fontanella, A., Mair, G., Wardlaw, J., Trucco, E., & Storkey, A. (2024). Diffusion models for counterfactual generation and anomaly detection in brain images. IEEE Transactions on Medical Imaging.](https://ieeexplore.ieee.org/abstract/document/10680156/)

[Chen, T., Wang, C., & Shan, H. (2023, October). Berdiff: Conditional bernoulli diffusion model for medical image segmentation. In International conference on medical image computing and computer-assisted intervention (pp. 491-501). Cham: Springer Nature Switzerland.](https://link.springer.com/chapter/10.1007/978-3-031-43901-8_47)

[Jiang, H., Luo, A., Liu, X., Han, S., & Liu, S. (2025). Lightendiffusion: Unsupervised low-light image enhancement with latent-retinex diffusion models. In European Conference on Computer Vision (pp. 161-179). Springer, Cham.](https://arxiv.org/pdf/2407.08939)

[Song, T., Wu, Y., Hu, M., Luo, X., Luo, G., Wang, G., ... & Zhang, S. (2024). Cycle-Consistent Bridge Diffusion Model for Accelerated MRI Reconstruction. arXiv preprint arXiv:2412.09998.](https://arxiv.org/pdf/2412.09998)

[Khazrak, I., Takhirova, S., Rezaee, M. M., Yadollahi, M., Green II, R. C., & Niu, S. (2024). Addressing Small and Imbalanced Medical Image Datasets Using Generative Models: A Comparative Study of DDPM and PGGANs with Random and Greedy K Sampling. arXiv preprint arXiv:2412.12532.](https://arxiv.org/pdf/2412.12532)

[Li, X., Ren, Y., Jin, X., Lan, C., Wang, X., Zeng, W., ... & Chen, Z. (2023). Diffusion Models for Image Restoration and Enhancement--A Comprehensive Survey. arXiv preprint arXiv:2308.09388.](https://arxiv.org/pdf/2308.09388)

[Sun, Y., Chen, Z., Zheng, H., Ge, R., Liu, J., Min, W., ... & Wang, C. (2024). BS-LDM: Effective Bone Suppression in High-Resolution Chest X-Ray Images with Conditional Latent Diffusion Models. arXiv preprint arXiv:2412.15670.](https://arxiv.org/pdf/2412.15670)





